@article{Lersch_2025,
doi = {10.1088/2632-2153/adc8fb},
url = {https://dx.doi.org/10.1088/2632-2153/adc8fb},
year = {2025},
month = {apr},
publisher = {IOP Publishing},
volume = {6},
number = {2},
pages = {025017},
author = {Lersch, Daniel and Schram, Malachi and Dai, Zhenyu and Rajput, Kishansingh and Sato, Nobuo and Wu, Xingfu and Childers, J Taylor and Goldenberg, Steven},
title = {SAGIPS: a physics-inspired scalable asynchronous generative inverse-problem solver},
journal = {Machine Learning: Science and Technology},
abstract = {Solving large-scale inverse problems using deep-learning algorithms have become an essential part of modern research and industrial applications. The complexity of the underlying inverse problem may require the utilization of high performance computing systems which poses a challenge on the algorithmic design of the inverse problem solver. Most deep learning algorithms require, due to their design, custom parallelization techniques in order to be resource efficient while showing a reasonable convergence. In this paper we introduce a Scalable Asynchronous Generative Inverse Problem Solver (SAGIPS) on high-performance computing systems. We present a workflow that utilizes an asynchronous ring-allreduce algorithm to transfer the gradients of the generator network across multiple GPUs. Experiments with a scientific proxy application demonstrate that SAGIPS shows near linear weak scaling, together with a convergence quality that is comparable to traditional methods. The approach presented here allows leveraging Generative Adverserial Network across multiple GPUs, promising advancements in solving complex inverse problems at scale.}
}
