@article{Rajput_2025,
doi = {10.1088/2632-2153/adc221},
url = {https://dx.doi.org/10.1088/2632-2153/adc221},
year = {2025},
month = {apr},
publisher = {IOP Publishing},
volume = {6},
number = {2},
pages = {025018},
author = {Rajput, Kishansingh and Schram, Malachi and Edelen, Auralee and Colen, Jonathan and Kasparian, Armen and Roussel, Ryan and Carpenter, Adam and Zhang, He and Benesch, Jay},
title = {Harnessing the power of gradient-based simulations for multi-objective optimization in particle accelerators},
journal = {Machine Learning: Science and Technology},
abstract = {Particle accelerator operation requires simultaneous optimization of multiple objectives. Multi-objective optimization (MOO) is particularly challenging due to trade-offs between the objectives. Evolutionary algorithms, such as genetic algorithms (GAs), have been leveraged for many optimization problems, however, they do not apply to complex control problems by design. This paper demonstrates the power of differentiability for solving MOO problems in particle accelerators using a deep differentiable reinforcement learning (DDRL) algorithm. We compare the DDRL algorithm with model-free reinforcement learning (MFRL), GA, and Bayesian optimization (BO) for simultaneous optimization of heat load and trip rates in the continuous electron beam accelerator facility. The underlying problem enforces strict constraints on both individual states and actions as well as cumulative (global) constraints on energy requirements of the beam. Using historical accelerator data, we develop a physics-based surrogate model which is differentiable and allows for back-propagation of gradients. The results are evaluated in the form of a Pareto-front with two objectives. We show that the DDRL outperforms MFRL, BO, and GA on high dimensional problems.}
}
